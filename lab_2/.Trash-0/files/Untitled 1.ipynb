{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e88d02-5c2e-4d18-856d-4d97ec122dd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 423\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# ============== ä½¿ç”¨ç¤ºä¾‹ ==============\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    421\u001b[0m     \n\u001b[1;32m    422\u001b[0m     \u001b[38;5;66;03m# åˆå§‹åŒ–\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     analyzer \u001b[38;5;241m=\u001b[39m FeatureEDA(\u001b[43mspark\u001b[49m)\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# è¿è¡Œpipeline - åªéœ€è¦æŒ‡å®šdataæ–‡ä»¶å¤¹è·¯å¾„\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     keep_features, eda_results, summary \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mrun_pipeline(\n\u001b[1;32m    427\u001b[0m         data_folder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType\n",
    "\n",
    "import utils.data_processing_bronze_table\n",
    "import utils.data_processing_silver_table\n",
    "import utils.data_processing_gold_table\n",
    "\n",
    "\n",
    "class FeatureEDA:\n",
    "    \"\"\"\n",
    "    åŸºäºä¸šåŠ¡é€»è¾‘çš„ç‰¹å¾ç­›é€‰ + EDA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        \n",
    "    def load_features(self, data_folder):\n",
    "        \"\"\"\n",
    "        ä»dataæ–‡ä»¶å¤¹åŠ è½½ä¸‰ä¸ªç‰¹å¾æ•°æ®é›†\n",
    "        \"\"\"\n",
    "        print(\"=\"*80)\n",
    "        print(\"STEP 1: Loading Feature Datasets\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # åŠ è½½clickstream\n",
    "        clickstream_path = os.path.join(data_folder, \"feature_clickstream.csv\")\n",
    "        print(f\"\\nLoading: {clickstream_path}\")\n",
    "        df_click = self.spark.read.option(\"header\", \"true\").csv(clickstream_path)\n",
    "        df_click = df_click.withColumn(\"snapshot_date\", \n",
    "                                       F.to_date(F.col(\"snapshot_date\"), \"yyyy/M/d\"))\n",
    "        print(f\"  âœ“ Clickstream: {df_click.count()} rows, {len(df_click.columns)} columns\")\n",
    "        print(f\"  Columns: {df_click.columns}\")\n",
    "        \n",
    "        # åŠ è½½attributes\n",
    "        attributes_path = os.path.join(data_folder, \"features_attributes.csv\")\n",
    "        print(f\"\\nLoading: {attributes_path}\")\n",
    "        df_attr = self.spark.read.option(\"header\", \"true\").csv(attributes_path)\n",
    "        df_attr = df_attr.withColumn(\"snapshot_date\", \n",
    "                                     F.to_date(F.col(\"snapshot_date\"), \"yyyy/M/d\"))\n",
    "        print(f\"  âœ“ Attributes: {df_attr.count()} rows, {len(df_attr.columns)} columns\")\n",
    "        print(f\"  Columns: {df_attr.columns}\")\n",
    "        \n",
    "        # åŠ è½½financials\n",
    "        financials_path = os.path.join(data_folder, \"features_financials.csv\")\n",
    "        print(f\"\\nLoading: {financials_path}\")\n",
    "        df_fin = self.spark.read.option(\"header\", \"true\").csv(financials_path)\n",
    "        df_fin = df_fin.withColumn(\"snapshot_date\", \n",
    "                                   F.to_date(F.col(\"snapshot_date\"), \"yyyy/M/d\"))\n",
    "        print(f\"  âœ“ Financials: {df_fin.count()} rows, {len(df_fin.columns)} columns\")\n",
    "        print(f\"  Columns: {df_fin.columns}\")\n",
    "        \n",
    "        return df_click, df_attr, df_fin\n",
    "    \n",
    "    def define_feature_categories(self, df_click, df_attr, df_fin):\n",
    "        \"\"\"\n",
    "        æ ¹æ®ä¸šåŠ¡é€»è¾‘åˆ†ç±»ç‰¹å¾\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: Categorizing Features by Business Logic\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # å®šä¹‰ç‰¹å¾åˆ†ç±»\n",
    "        feature_categories = {\n",
    "            'âŒ EXCLUDE - Identity/Name': [\n",
    "                'Name', 'SSN'  # ä¸ªäººè¯†åˆ«ä¿¡æ¯ï¼Œæ— é¢„æµ‹ä»·å€¼\n",
    "            ],\n",
    "            \n",
    "            'âŒ EXCLUDE - Data Leakage': [\n",
    "                'Num_of_Delayed_Payment',  # å»¶è¿Ÿè¿˜æ¬¾æ¬¡æ•° - å·²ç»é€¾æœŸäº†\n",
    "                'Delay_from_due_date',      # è·ç¦»åˆ°æœŸæ—¥å»¶è¿Ÿå¤©æ•° - å·²ç»é€¾æœŸ\n",
    "                'Payment_of_Min_Amount',    # æ˜¯å¦æ”¯ä»˜æœ€ä½è¿˜æ¬¾ - å¯èƒ½åŒ…å«æœªæ¥ä¿¡æ¯\n",
    "            ],\n",
    "            \n",
    "            'âœ… KEEP - Demographic': [\n",
    "                'Age',          # å¹´é¾„\n",
    "                'Occupation',   # èŒä¸š\n",
    "            ],\n",
    "            \n",
    "            'âœ… KEEP - Financial Capacity': [\n",
    "                'Annual_Income',           # å¹´æ”¶å…¥\n",
    "                'Monthly_Inhand_Salary',   # æœˆåˆ°æ‰‹å·¥èµ„\n",
    "                'Monthly_Balance',         # æœˆä½™é¢\n",
    "                'Amount_invested_monthly', # æœˆæŠ•èµ„é‡‘é¢\n",
    "            ],\n",
    "            \n",
    "            'âœ… KEEP - Credit Behavior': [\n",
    "                'Num_Bank_Accounts',      # é“¶è¡Œè´¦æˆ·æ•°\n",
    "                'Num_Credit_Card',        # ä¿¡ç”¨å¡æ•°é‡\n",
    "                'Interest_Rate',          # åˆ©ç‡\n",
    "                'Num_of_Loan',            # è´·æ¬¾æ•°é‡\n",
    "                'Type_of_Loan',           # è´·æ¬¾ç±»å‹\n",
    "                'Changed_Credit_Limit',   # ä¿¡ç”¨é¢åº¦å˜åŒ–\n",
    "                'Num_Credit_Inquiries',   # ä¿¡ç”¨æŸ¥è¯¢æ¬¡æ•°\n",
    "                'Credit_Mix',             # ä¿¡ç”¨ç»„åˆ\n",
    "                'Outstanding_Debt',       # æœªå¿å€ºåŠ¡\n",
    "                'Credit_Utilization_Ratio', # ä¿¡ç”¨ä½¿ç”¨ç‡\n",
    "                'Credit_History_Age',     # ä¿¡ç”¨å†å²å¹´é¾„\n",
    "            ],\n",
    "            \n",
    "            'âœ… KEEP - Payment Behavior': [\n",
    "                'Total_EMI_per_month',    # æœˆæ€»EMI\n",
    "                'Payment_Behaviour',       # æ”¯ä»˜è¡Œä¸ºæ¨¡å¼\n",
    "            ],\n",
    "            \n",
    "            'âœ… KEEP - Clickstream': []  # æ‰€æœ‰fe_å¼€å¤´çš„ç‰¹å¾\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ æ‰€æœ‰clickstreamç‰¹å¾\n",
    "        clickstream_features = [col for col in df_click.columns \n",
    "                               if col.startswith('fe_')]\n",
    "        feature_categories['âœ… KEEP - Clickstream'] = clickstream_features\n",
    "        \n",
    "        # æ”¶é›†æ‰€æœ‰è¦ä¿ç•™çš„ç‰¹å¾\n",
    "        keep_features = []\n",
    "        exclude_features = []\n",
    "        \n",
    "        print(\"\\nğŸ“‹ Feature Categories:\")\n",
    "        for category, features in feature_categories.items():\n",
    "            print(f\"\\n{category} ({len(features)} features):\")\n",
    "            for feat in features:\n",
    "                print(f\"  - {feat}\")\n",
    "            \n",
    "            if category.startswith('âœ…'):\n",
    "                keep_features.extend(features)\n",
    "            else:\n",
    "                exclude_features.extend(features)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"âœ… Total features to KEEP: {len(keep_features)}\")\n",
    "        print(f\"âŒ Total features to EXCLUDE: {len(exclude_features)}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return keep_features, exclude_features, feature_categories\n",
    "    \n",
    "    def comprehensive_eda(self, df_click, df_attr, df_fin, keep_features):\n",
    "        \"\"\"\n",
    "        å¯¹ä¿ç•™çš„ç‰¹å¾è¿›è¡Œè¯¦ç»†EDA\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: Comprehensive EDA for Selected Features\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        eda_results = {}\n",
    "        \n",
    "        # åˆå¹¶æ‰€æœ‰æ•°æ®é›†ç”¨äºåˆ†æ\n",
    "        all_dfs = {\n",
    "            'clickstream': df_click,\n",
    "            'attributes': df_attr,\n",
    "            'financials': df_fin\n",
    "        }\n",
    "        \n",
    "        for dataset_name, df in all_dfs.items():\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ğŸ“Š Analyzing {dataset_name.upper()}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # è·å–è¯¥æ•°æ®é›†ä¸­çš„ä¿ç•™ç‰¹å¾\n",
    "            dataset_features = [col for col in df.columns \n",
    "                              if col in keep_features]\n",
    "            \n",
    "            if len(dataset_features) == 0:\n",
    "                print(f\"  No features to analyze in this dataset\")\n",
    "                continue\n",
    "            \n",
    "            dataset_results = []\n",
    "            \n",
    "            for idx, col in enumerate(dataset_features, 1):\n",
    "                print(f\"\\n[{idx}/{len(dataset_features)}] {col}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                feature_info = {\n",
    "                    'feature_name': col,\n",
    "                    'dataset': dataset_name\n",
    "                }\n",
    "                \n",
    "                # åŸºç¡€ç»Ÿè®¡\n",
    "                total = df.count()\n",
    "                null_count = df.filter(F.col(col).isNull() | (F.col(col) == \"\")).count()\n",
    "                null_rate = (null_count / total) * 100\n",
    "                unique_count = df.select(col).distinct().count()\n",
    "                \n",
    "                feature_info['total_records'] = total\n",
    "                feature_info['null_count'] = null_count\n",
    "                feature_info['null_rate'] = f\"{null_rate:.2f}%\"\n",
    "                feature_info['unique_count'] = unique_count\n",
    "                \n",
    "                # è·å–æ ·æœ¬å€¼\n",
    "                samples = df.select(col).limit(10).toPandas()[col].tolist()\n",
    "                feature_info['sample_values'] = samples\n",
    "                \n",
    "                print(f\"  Total records: {total:,}\")\n",
    "                print(f\"  Missing: {null_count:,} ({null_rate:.2f}%)\")\n",
    "                print(f\"  Unique values: {unique_count:,}\")\n",
    "                print(f\"  Sample values: {samples[:5]}\")\n",
    "                \n",
    "                # åˆ¤æ–­æ•°æ®ç±»å‹\n",
    "                if len(samples) > 0:\n",
    "                    sample_non_null = df.filter(F.col(col).isNotNull() & (F.col(col) != \"\")) \\\n",
    "                                       .select(col).limit(100).toPandas()[col]\n",
    "                    \n",
    "                    if len(sample_non_null) > 0:\n",
    "                        # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼å‹\n",
    "                        cleaned = sample_non_null.astype(str).str.replace('[^0-9.-]', '', regex=True)\n",
    "                        numeric_count = cleaned.str.match(r'^-?\\d*\\.?\\d+$').sum()\n",
    "                        is_numeric = (numeric_count / len(sample_non_null)) > 0.7\n",
    "                        \n",
    "                        if is_numeric:\n",
    "                            feature_info['data_type'] = 'numeric'\n",
    "                            \n",
    "                            # æ¸…æ´—å¹¶è®¡ç®—æ•°å€¼ç»Ÿè®¡\n",
    "                            df_clean = df.withColumn(\n",
    "                                f\"{col}_clean\",\n",
    "                                F.regexp_replace(F.col(col), \"[^0-9.-]\", \"\")\n",
    "                            )\n",
    "                            df_clean = df_clean.withColumn(\n",
    "                                f\"{col}_clean\",\n",
    "                                F.when(F.col(f\"{col}_clean\").rlike(\"^-?[0-9]*\\\\.?[0-9]+$\"), \n",
    "                                      F.col(f\"{col}_clean\").cast(DoubleType()))\n",
    "                                .otherwise(None)\n",
    "                            )\n",
    "                            \n",
    "                            stats = df_clean.select(\n",
    "                                F.min(f\"{col}_clean\").alias('min'),\n",
    "                                F.max(f\"{col}_clean\").alias('max'),\n",
    "                                F.mean(f\"{col}_clean\").alias('mean'),\n",
    "                                F.stddev(f\"{col}_clean\").alias('stddev'),\n",
    "                                F.expr(f\"percentile_approx(`{col}_clean`, 0.25)\").alias('q25'),\n",
    "                                F.expr(f\"percentile_approx(`{col}_clean`, 0.50)\").alias('median'),\n",
    "                                F.expr(f\"percentile_approx(`{col}_clean`, 0.75)\").alias('q75')\n",
    "                            ).collect()[0]\n",
    "                            \n",
    "                            feature_info['statistics'] = {\n",
    "                                'min': float(stats['min']) if stats['min'] is not None else None,\n",
    "                                'max': float(stats['max']) if stats['max'] is not None else None,\n",
    "                                'mean': float(stats['mean']) if stats['mean'] is not None else None,\n",
    "                                'stddev': float(stats['stddev']) if stats['stddev'] is not None else None,\n",
    "                                'q25': float(stats['q25']) if stats['q25'] is not None else None,\n",
    "                                'median': float(stats['median']) if stats['median'] is not None else None,\n",
    "                                'q75': float(stats['q75']) if stats['q75'] is not None else None\n",
    "                            }\n",
    "                            \n",
    "                            print(f\"  Type: NUMERIC\")\n",
    "                            if stats['min'] is not None:\n",
    "                                print(f\"    Min: {stats['min']:.2f}\")\n",
    "                                print(f\"    Max: {stats['max']:.2f}\")\n",
    "                                print(f\"    Mean: {stats['mean']:.2f}\")\n",
    "                                print(f\"    Median: {stats['median']:.2f}\")\n",
    "                                print(f\"    Std: {stats['stddev']:.2f}\")\n",
    "                                \n",
    "                                # æ•°æ®è´¨é‡æç¤º\n",
    "                                if null_rate > 30:\n",
    "                                    print(f\"  âš ï¸  WARNING: High missing rate!\")\n",
    "                                if stats['stddev'] and stats['stddev'] < 0.01:\n",
    "                                    print(f\"  âš ï¸  WARNING: Very low variance!\")\n",
    "                            \n",
    "                        else:\n",
    "                            feature_info['data_type'] = 'categorical'\n",
    "                            \n",
    "                            # åˆ†ç±»å˜é‡ç»Ÿè®¡\n",
    "                            top_values = df.groupBy(col).count() \\\n",
    "                                          .orderBy(F.desc(\"count\")) \\\n",
    "                                          .limit(10) \\\n",
    "                                          .toPandas()\n",
    "                            \n",
    "                            feature_info['top_values'] = top_values.to_dict('records')\n",
    "                            \n",
    "                            print(f\"  Type: CATEGORICAL\")\n",
    "                            print(f\"  Top 10 values:\")\n",
    "                            for _, row in top_values.iterrows():\n",
    "                                pct = (row['count'] / total) * 100\n",
    "                                val = row[col] if row[col] else \"[NULL/EMPTY]\"\n",
    "                                print(f\"    {val}: {row['count']:,} ({pct:.1f}%)\")\n",
    "                            \n",
    "                            # æ•°æ®è´¨é‡æç¤º\n",
    "                            if unique_count > 100:\n",
    "                                print(f\"  âš ï¸  WARNING: High cardinality!\")\n",
    "                            if null_rate > 30:\n",
    "                                print(f\"  âš ï¸  WARNING: High missing rate!\")\n",
    "                \n",
    "                dataset_results.append(feature_info)\n",
    "            \n",
    "            eda_results[dataset_name] = dataset_results\n",
    "        \n",
    "        return eda_results\n",
    "    \n",
    "    def generate_summary(self, eda_results, keep_features, exclude_features, feature_categories):\n",
    "        \"\"\"\n",
    "        ç”ŸæˆEDAæ€»ç»“æŠ¥å‘Š\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 4: EDA Summary\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ç»Ÿè®¡ç‰¹å¾ç±»å‹\n",
    "        all_features = []\n",
    "        for dataset in eda_results.values():\n",
    "            all_features.extend(dataset)\n",
    "        \n",
    "        numeric_features = [f for f in all_features if f.get('data_type') == 'numeric']\n",
    "        categorical_features = [f for f in all_features if f.get('data_type') == 'categorical']\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Feature Type Distribution:\")\n",
    "        print(f\"  Total selected features: {len(keep_features)}\")\n",
    "        print(f\"  Numeric features: {len(numeric_features)}\")\n",
    "        print(f\"  Categorical features: {len(categorical_features)}\")\n",
    "        print(f\"  Excluded features: {len(exclude_features)}\")\n",
    "        \n",
    "        # æ•°æ®è´¨é‡é—®é¢˜\n",
    "        print(f\"\\nâš ï¸  Data Quality Issues:\")\n",
    "        high_missing = [f for f in all_features \n",
    "                       if float(f['null_rate'].rstrip('%')) > 30]\n",
    "        print(f\"  High missing rate (>30%): {len(high_missing)}\")\n",
    "        if high_missing:\n",
    "            for f in high_missing:\n",
    "                print(f\"    - {f['feature_name']}: {f['null_rate']}\")\n",
    "        \n",
    "        high_cardinality = [f for f in categorical_features \n",
    "                           if f['unique_count'] > 100]\n",
    "        print(f\"  High cardinality categorical (>100 unique): {len(high_cardinality)}\")\n",
    "        if high_cardinality:\n",
    "            for f in high_cardinality:\n",
    "                print(f\"    - {f['feature_name']}: {f['unique_count']:,} unique values\")\n",
    "        \n",
    "        # ç‰¹å¾åˆ†å¸ƒæ¦‚è§ˆ\n",
    "        print(f\"\\nğŸ“‹ Feature Distribution by Category:\")\n",
    "        for category, features in feature_categories.items():\n",
    "            if category.startswith('âœ…'):\n",
    "                print(f\"  {category}: {len(features)}\")\n",
    "        \n",
    "        summary = {\n",
    "            'total_selected': len(keep_features),\n",
    "            'total_excluded': len(exclude_features),\n",
    "            'numeric_count': len(numeric_features),\n",
    "            'categorical_count': len(categorical_features),\n",
    "            'high_missing_count': len(high_missing),\n",
    "            'high_cardinality_count': len(high_cardinality),\n",
    "            'feature_categories': feature_categories\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_results(self, keep_features, exclude_features, eda_results, summary):\n",
    "        \"\"\"\n",
    "        ä¿å­˜åˆ†æç»“æœ\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 5: Saving Results\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # ä¿å­˜ç­›é€‰åçš„ç‰¹å¾åˆ—è¡¨\n",
    "        with open(\"selected_features.txt\", \"w\") as f:\n",
    "            f.write(\"# Selected Features for Modeling\\n\")\n",
    "            f.write(f\"# Total: {len(keep_features)} features\\n\\n\")\n",
    "            for feat in keep_features:\n",
    "                f.write(f\"{feat}\\n\")\n",
    "        print(\"âœ“ Saved: selected_features.txt\")\n",
    "        \n",
    "        # ä¿å­˜æ’é™¤çš„ç‰¹å¾åˆ—è¡¨\n",
    "        with open(\"excluded_features.txt\", \"w\") as f:\n",
    "            f.write(\"# Excluded Features\\n\")\n",
    "            f.write(f\"# Total: {len(exclude_features)} features\\n\\n\")\n",
    "            for feat in exclude_features:\n",
    "                f.write(f\"{feat}\\n\")\n",
    "        print(\"âœ“ Saved: excluded_features.txt\")\n",
    "        \n",
    "        # ä¿å­˜è¯¦ç»†EDAæŠ¥å‘Š\n",
    "        import json\n",
    "        with open(\"eda_report.json\", \"w\") as f:\n",
    "            json.dump(eda_results, f, indent=2, default=str)\n",
    "        print(\"âœ“ Saved: eda_report.json\")\n",
    "        \n",
    "        # ä¿å­˜æ€»ç»“\n",
    "        with open(\"eda_summary.json\", \"w\") as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        print(\"âœ“ Saved: eda_summary.json\")\n",
    "        \n",
    "        print(\"\\nâœ… All results saved successfully!\")\n",
    "    \n",
    "    def run_pipeline(self, data_folder):\n",
    "        \"\"\"\n",
    "        è¿è¡Œå®Œæ•´pipeline\n",
    "        \"\"\"\n",
    "        # Step 1: åŠ è½½æ•°æ®\n",
    "        df_click, df_attr, df_fin = self.load_features(data_folder)\n",
    "        \n",
    "        # Step 2: æ ¹æ®ä¸šåŠ¡é€»è¾‘ç­›é€‰ç‰¹å¾\n",
    "        keep_features, exclude_features, feature_categories = \\\n",
    "            self.define_feature_categories(df_click, df_attr, df_fin)\n",
    "        \n",
    "        # Step 3: å¯¹ä¿ç•™ç‰¹å¾åšEDA\n",
    "        eda_results = self.comprehensive_eda(df_click, df_attr, df_fin, keep_features)\n",
    "        \n",
    "        # Step 4: ç”Ÿæˆæ€»ç»“\n",
    "        summary = self.generate_summary(eda_results, keep_features, exclude_features, feature_categories)\n",
    "        \n",
    "        # Step 5: ä¿å­˜ç»“æœ\n",
    "        self.save_results(keep_features, exclude_features, eda_results, summary)\n",
    "        \n",
    "        return keep_features, eda_results, summary\n",
    "\n",
    "\n",
    "# ============== ä½¿ç”¨ç¤ºä¾‹ ==============\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # åˆå§‹åŒ–\n",
    "    analyzer = FeatureEDA(spark)\n",
    "    \n",
    "    # è¿è¡Œpipeline - åªéœ€è¦æŒ‡å®šdataæ–‡ä»¶å¤¹è·¯å¾„\n",
    "    keep_features, eda_results, summary = analyzer.run_pipeline(\n",
    "        data_folder=\"data\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ‰ Pipeline Completed!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nğŸ“ Generated Files:\")\n",
    "    print(f\"  1. selected_features.txt - List of features to use\")\n",
    "    print(f\"  2. excluded_features.txt - List of excluded features\")\n",
    "    print(f\"  3. eda_report.json - Detailed EDA for each feature\")\n",
    "    print(f\"  4. eda_summary.json - Summary statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b8342-a0df-43e3-bed2-fc54789d3510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
